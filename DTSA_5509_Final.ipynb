{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"import-libraries\"></a>\n",
    "# Import necessary libraries for data preprocessing, visualization, and modeling\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "361b490ce02eafdd"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib as mpl # For changing some default matplotlib parameters\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np   # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For data visualization\n",
    "import matplotlib.colors as colors  # For customizing plot colors\n",
    "import seaborn as sns  # For data visualization\n",
    "import warnings # To ignore some matplotlib warnings\n",
    "warnings.filterwarnings('ignore')  # To ignore some matplotlib warnings\n",
    "from sklearn.model_selection import train_test_split  # For splitting the dataset into train and test sets\n",
    "from sklearn import preprocessing  # For preprocessing data (e.g., scaling)\n",
    "from sklearn.svm import SVC  # Support Vector Machine classifier\n",
    "from sklearn.model_selection import GridSearchCV  # For hyperparameter tuning using grid search\n",
    "from sklearn.metrics import ConfusionMatrixDisplay  # For displaying confusion matrices\n",
    "from sklearn.decomposition import PCA  # Principal Component Analysis for dimensionality reduction\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  # For model evaluation\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.730454Z"
    }
   },
   "id": "5dd550851337b622"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"download-the-data\"></a>\n",
    "# Import the data\n",
    "Now we load in a dataset from the **[UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)**.\n",
    "Specifically, we are going to use the **[Heart Disease Dataset](https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data/download?datasetVersionNumber=6)**. This dataset will allow us to predict if someone has heart disease based on their sex, age, blood pressure and a variety of other metrics. The dataset is stored in a CSV file, which we can load into a Pandas dataframe using the `read_csv` function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56c5809f0c5acd84"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "heart_data = pd.read_csv('https://raw.githubusercontent.com/marcofanti/DTSA_5509_Final/main/data/heart_disease_uci.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.731438Z"
    }
   },
   "id": "5b28683fa0d4922f"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "heart_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.732367Z"
    }
   },
   "id": "161ddd946b06231a"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "heart_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.733624Z"
    }
   },
   "id": "a35f148bb560af11"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "heart_data.info()\n",
    "\n",
    "heart_data['trestbps'].unique()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.735032Z"
    }
   },
   "id": "d39db4f051b02fea"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "heart_data['dataset'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.737023Z"
    }
   },
   "id": "cfdddef48d7c5bc2"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "heart_data['fbs'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.738366Z"
    }
   },
   "id": "d3f5293bfd7c61db"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "heart_data['ca'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.739774Z"
    }
   },
   "id": "2fdfabca0d0c46bd"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "heart_data['thal'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.740918Z"
    }
   },
   "id": "5b1a7097675ad728"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "len(heart_data.loc[(heart_data['ca'].isna()) | (heart_data['thal'].isna())| (heart_data['fbs'].isna()) | (heart_data['trestbps'].isna())])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.742284Z"
    }
   },
   "id": "2f567e44b19b5882"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "#Since we have 621 rows with missing values, we will drop them\n",
    "heart_data_no_missing = heart_data.dropna(subset=['ca', 'thal', 'fbs', 'trestbps'])\n",
    "heart_data_no_missing.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.743716Z"
    }
   },
   "id": "7f4d24b476473667"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "heart_data_no_missing['dataset'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.745091Z"
    }
   },
   "id": "84559fdd73701fea"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "len(heart_data_no_missing.loc[(heart_data_no_missing['dataset'] == 'Cleveland')])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.746517Z"
    }
   },
   "id": "c744c40422c599df"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "# Since 299 rows have dataset = 'Cleveland', we will drop the others (Hungarian, and VA Long Beach)\n",
    "# We could have done this earlier and that would have avoided a lot of the missing values\n",
    "heart_data_no_missing = heart_data_no_missing.loc[(heart_data_no_missing['dataset'] == 'Cleveland')]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.747784Z"
    }
   },
   "id": "cdad63f3d6f663df"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "heart_data_no_missing.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.748729Z"
    }
   },
   "id": "f69aafb1a8720af9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"format-the-data\"></a>\n",
    "# Format Data Part 1: Split the Data into Dependent and Independent Variables\n",
    "\n",
    "## Data Preparation for Support Vector Machine (SVM)\n",
    "\n",
    "Now that we have handled the missing data, our next step is to format the data for building a Support Vector Machine (SVM) model.\n",
    "\n",
    "### Data Splitting\n",
    "\n",
    "The first step in this process involves splitting the dataset into two parts:\n",
    "1. The columns of data that will be used for making classifications (referred to as **X**).\n",
    "2. The column of data that we want to predict, in this case, it's **hd** (heart disease), which will be represented as **y**.\n",
    "\n",
    "It's important to note that dealing with missing data before splitting ensures that each row in **X** corresponds correctly with the appropriate value in **y**.\n",
    "\n",
    "### Copying Data\n",
    "\n",
    "In the code below, we use the `copy()` method to create a copy of the data *by value* instead of *by reference*. This ensures that the original data `df_no_missing` remains unaltered when we manipulate `X` or `y`. This approach allows us to experiment with formatting columns for classification without affecting the original dataset. If any mistakes occur during the process, we can simply re-copy `df_no_missing` rather than reloading the original data and reapplying missing value handling.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8092a7c1d6bd71b8"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "X = heart_data_no_missing.drop('num', axis=1).drop('dataset', axis=1).copy() # alternatively: X = heart_data_no_missing.iloc[:,:-1].copy()\n",
    "X.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.749940Z"
    }
   },
   "id": "884f9ca3afa9ea48"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "y = heart_data_no_missing['num'].copy()\n",
    "y.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.751314Z"
    }
   },
   "id": "35922b170bbd3458"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Format Data Part 2: Convert Categorical Data to Dummy Variables\n",
    "\n",
    "Why Convert Categorical Data?\n",
    "\n",
    "Many machine learning algorithms can only handle numerical data. Categorical data, however, is often represented by non-numerical labels, making it incompatible with these algorithms directly. Converting categorical data to dummy variables solves this issue by creating new binary features for each category within the original variable.\n",
    "\n",
    "Benefits of Dummy Variables:\n",
    "\n",
    "Improved Model Performance: By converting categorical data to numerical representation, models can efficiently analyze and learn from the information contained within these features.\n",
    "Interpretability: Dummy variables provide clear information about the presence or absence of each category, aiding in model interpretation and understanding the influence of categorical features on predictions.\n",
    "Consistency: By using a consistent numerical representation, different models can handle categorical data in a similar manner, facilitating comparisons and analyses.\n",
    "Types of Dummy Variables:\n",
    "\n",
    "One-Hot Encoding: This method creates a new binary feature for each category. The feature is set to 1 for instances belonging to that category and 0 otherwise.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f93b4eaff4b42f54"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "X_encoded = pd.get_dummies(X, columns=['sex', \n",
    "                                       'cp',\n",
    "                                       'restecg',\n",
    "                                       'slope',\n",
    "                                       'thal'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.752874Z"
    }
   },
   "id": "dc1517207fca446"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "X_encoded.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.753902Z"
    }
   },
   "id": "5a1a49ed7a100997"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "y.unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.755166Z"
    }
   },
   "id": "ef9c2848ded03758"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In doing heart disease prediction, we only care about classifying individuals as having or not having the disease. This necessitates a simplified target variable representation.\n",
    "\n",
    "Therefore, we'll convert all values in the target variable > 0 to 1. This effectively translates any degree of heart disease into a binary presence/absence indicator, making it suitable for binary classification capabilities."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a580542d888d66c"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "y_not_zero_idx = y > 0  # get the indices for which y is not zero\n",
    "y[y_not_zero_idx] = 1  # set y at those indices to 1\n",
    "y.unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.756505Z"
    }
   },
   "id": "2db95e57848f6f59"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Univariate Selection for Feature Selection\n",
    "\n",
    "In feature selection, univariate statistical tests can help identify features that have the strongest relationships with the target variable. The scikit-learn library offers the `SelectKBest` class, which allows us to select a specific number of features using various statistical tests.\n",
    "\n",
    "For instance, in the following example, we utilize the chi-squared (`chi2`) statistical test, designed for non-negative features, to select the top 10 features with the most significant relationships with the target variable from the dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4eac2ac49c57a64"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "X_drop = X_encoded.drop(columns=['id']).copy()\n",
    "# Select features\n",
    "selector = SelectKBest(score_func=chi2, k=10) # Select 10 best features\n",
    "\n",
    "fit = selector.fit(X_drop,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X_drop.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score'))  #print best features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.757956Z"
    }
   },
   "id": "2eff4a2c4cb7513e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlation Matrix with Heatmap\n",
    "\n",
    "A correlation matrix provides insights into how features are related to each other and to the target variable. Correlations can be either positive (an increase in one feature corresponds to an increase in the target variable) or negative (an increase in one feature corresponds to a decrease in the target variable).\n",
    "\n",
    "Using a heatmap, we can easily visualize the features that are most relevant to the target variable. To create the heatmap, we'll utilize the seaborn library.\n",
    "\n",
    "Correlation helps us understand the relationships between features and the target variable. Positive correlations imply that an increase in a feature corresponds to an increase in the target variable, while negative correlations imply the opposite.\n",
    "\n",
    "From the heatmap, we can observe that the 'cp' (chest pain) feature exhibits a strong positive correlation with the target variable. This suggests that chest pain has a significant influence on predicting the presence of heart disease. In comparison to the relationships between other variables, we can conclude that chest pain is the most important factor in predicting the presence of heart disease.\n",
    "\n",
    "A medical emergency, such as a heart attack, typically occurs when a blood clot obstructs blood flow to the heart. This leads to a decrease in oxygen supply to the heart tissue, causing chest pain.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3096956c330defb4"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(X_drop.corr(),annot=True,cmap=\"YlGnBu\",fmt='.1f')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.759152Z"
    }
   },
   "id": "38e265dfe9ed14b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"center-and-scale\"></a>\n",
    "# Data Formatting Part 3: Centering and Scaling\n",
    "\n",
    "For our **Support Vector Machine (SVM)**, we are utilizing the **Radial Basis Function (RBF)**, which assumes that the data are centered and scaled. Therefore, it is necessary to perform centering and scaling on both the training and testing datasets.\n",
    "\n",
    "**IMPORTANT:** To prevent **Data Leakage**, we split the data into training and testing datasets before scaling them. **Data Leakage** can occur when information from the training dataset contaminates or influences the testing dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aef85a181471a4b9"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, random_state=42)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.762651Z"
    }
   },
   "id": "afeac29f458b633d"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "X_train_drop = X_train.drop(columns=['id']).copy()\n",
    "sns.heatmap(X_train_drop.corr(),annot=True,cmap=\"YlGnBu\",fmt='.1f')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.763578Z"
    }
   },
   "id": "3f31493c21b9a455"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "heart_data_no_missing_2 = heart_data_no_missing.drop(columns=['id']).copy()\n",
    "def chng2(prob):\n",
    "    if prob == 0:\n",
    "        return 'No Heart Disease'\n",
    "    else:\n",
    "        return 'Heart Disease'\n",
    "heart_data_no_missing_2['target'] = heart_data_no_missing_2['num'].apply(chng2)\n",
    "sns.countplot(data= heart_data_no_missing_2, x='sex',hue='target')\n",
    "plt.title('Gender v/s target\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.767284Z"
    }
   },
   "id": "78d753369327013"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gender and Heart Disease\n",
    "\n",
    "In the Cleveland dataset, it is evident that males are more susceptible to developing heart disease compared to females. Men have a higher incidence of heart attacks than women. Sudden heart attacks are experienced by a significant proportion of men, ranging from 70% to 89%.\n",
    "\n",
    "In contrast, women may experience heart attacks without the typical symptom of chest pressure. Instead, they often report symptoms such as nausea or vomiting, which can be easily mistaken for acid reflux or flu-like symptoms.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab402886a5a0e9d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Types of Chest Pain and Asymptomatic Heart Attacks\n",
    "\n",
    "In the context of heart disease, there are four types of chest pain:\n",
    "\n",
    "1. **Asymptomatic Chest Pain:** Most heart disease patients are found to have asymptomatic chest pain. These individuals may display atypical symptoms such as indigestion, flu-like symptoms, or the sensation of a strained chest muscle. Despite being asymptomatic, this type of chest pain involves the blockage of blood flow to the heart, potentially causing damage to the heart muscle.\n",
    "\n",
    "2. **Atypical Angina:** This type of chest pain is characterized by symptoms that do not fit the typical pattern of angina. It may be experienced as discomfort or pain in the chest, but the symptoms differ from the classic angina presentation.\n",
    "\n",
    "3. **Non-Anginal Pain:** Non-anginal chest pain refers to chest discomfort that is not related to the heart or angina. It can be caused by various factors unrelated to cardiac issues.\n",
    "\n",
    "4. **Typical Angina:** Typical angina is chest pain that follows a typical pattern and is usually related to heart-related issues.\n",
    "\n",
    "Risk factors for asymptomatic heart attacks are similar to those associated with heart symptoms. These factors include:\n",
    "\n",
    "- Age\n",
    "- Diabetes\n",
    "- Excess weight\n",
    "- Family history of heart disease\n",
    "- High blood pressure\n",
    "- High cholesterol\n",
    "- Lack of exercise\n",
    "- Prior heart attack\n",
    "- Tobacco use\n",
    "\n",
    "It's important to note that an asymptomatic heart attack puts individuals at a greater risk of experiencing another heart attack, which can be life-threatening. Additionally, having another heart attack increases the risk of complications, such as heart failure.\n",
    "\n",
    "Currently, there are no specific tests to predict the potential for an asymptomatic heart attack. The only way to confirm if an asymptomatic heart attack has occurred is through diagnostic tests such as an electrocardiogram (ECG) or echocardiogram, which can reveal changes indicating a previous heart attack.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5e33aeb14be6744"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "sns.countplot(data= heart_data_no_missing_2, x='sex',hue='thal')\n",
    "plt.title('Gender v/s Thalassemia\\n')\n",
    "print('Thalassemia (thal-uh-SEE-me-uh) is an inherited blood disorder that causes your body to have less hemoglobin than normal. Hemoglobin enables red blood cells to carry oxygen')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.768356Z"
    }
   },
   "id": "b73971198c6e2f60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Beta Thalassemia Cardiomyopathy\n",
    "\n",
    "Beta thalassemia cardiomyopathy is primarily characterized by two distinct phenotypes:\n",
    "\n",
    "1. **Dilated Type:** This type is characterized by left ventricular dilatation and impaired contractility.\n",
    "\n",
    "2. **Restrictive Type:** The restrictive phenotype involves restrictive left ventricular filling, pulmonary hypertension, and right heart failure.\n",
    "\n",
    "Individuals with severe thalassemia can experience various heart-related issues, including congestive heart failure and abnormal heart rhythms."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f43a4389b04bc1d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Auxiliary function\n",
    "# Compiles the results of many machine learning classifiers into a single dataframe for analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ca09f552aeca8d8"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "def add_to_classifier_summary_df(classifier_summary_df, classifier, classifier_name, X_test, y_test):\n",
    "    # Compiles the results of many machine learning classifiers into a single dataframe for analysis\n",
    "    if classifier_summary_df is None:\n",
    "        classifier_summary_df = pd.DataFrame(columns=[\"Classifier Name\", \"Accuracy\", \"Precision\", \"Recall\", \"Macro F1\"])\n",
    "\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "    classifier_summary_entry = {\"Classifier Name\": [classifier_name],\n",
    "                          \"Accuracy\": [accuracy_score(y_test, y_test_pred)],\n",
    "                          \"Precision\": [precision_score(y_test, y_test_pred, average=\"macro\")],\n",
    "                          \"Recall\": [recall_score(y_test, y_test_pred, average=\"macro\")],\n",
    "                          \"Macro F1\": [f1_score(y_test, y_test_pred, average=\"macro\")],\n",
    "                          }\n",
    "\n",
    "    return pd.concat([classifier_summary_df, pd.DataFrame(classifier_summary_entry)], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.769939Z"
    }
   },
   "id": "e6a953eab8221f80"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a Pairplot with Seaborn\n",
    "\n",
    "In data analysis and visualization, a pairplot is a valuable tool for exploring relationships between multiple variables in a dataset. We can create a pairplot using the `seaborn.pairplot` function, which is a part of the Seaborn library.\n",
    "\n",
    "A pairplot generates a grid of scatterplots, where each variable in the dataset is compared to every other variable. The main diagonal of the grid typically displays histograms or kernel density estimates for each variable, showing the distribution of individual features.\n",
    "\n",
    "Here's what a pairplot can help us achieve:\n",
    "\n",
    "1. **Scatterplots:** The off-diagonal cells of the grid contain scatterplots, which allow us to visualize the relationship between pairs of variables. Each point in a scatterplot represents a data point, and the position of the point on the plot indicates the values of the two variables being compared. Scatterplots help us identify patterns, correlations, and potential outliers in the data.\n",
    "\n",
    "2. **Diagonal Plots:** The diagonal cells contain histograms (or kernel density estimates) for each variable. These histograms provide insights into the distribution of each individual feature, helping us understand its central tendency and spread.\n",
    "\n",
    "By examining the scatterplots and diagonal plots collectively, we can gain a comprehensive understanding of how variables relate to each other and their individual distributions. Pairplots are particularly useful for initial data exploration, identifying potential correlations, and selecting relevant features for further analysis or modeling.\n",
    "\n",
    "Let's use the `seaborn.pairplot` function to create a pairplot and explore the relationships between variables in our dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f00bc643ef59195"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "#sns.pairplot(heart_data_no_missing_2,hue='cp')\n",
    "#plt.savefig('images/pairplot.png')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.770814Z"
    }
   },
   "id": "cc8bb13ba91988a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Pairplot](images/pairplot.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c198d1a05a4fe48"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic Regression is a fundamental and widely used statistical technique in the field of machine learning. It is primarily used for binary classification tasks, where the goal is to predict one of two possible outcomes (e.g., yes/no, true/false, 1/0) based on input features. Let's dive into the details and expand on the concept of Logistic Regression:\n",
    "\n",
    "### Mathematical Foundation\n",
    "- At its core, Logistic Regression uses the logistic function (sigmoid function) to model the relationship between input features and the probability of a binary outcome. The logistic function maps any real-valued number to a value between 0 and 1, making it suitable for estimating probabilities.\n",
    "\n",
    "### Hypothesis Function\n",
    "- In Logistic Regression, we formulate a hypothesis function that takes the form of the logistic function. It calculates the probability that a given input instance belongs to the positive class (class 1).\n",
    "\n",
    "    ### Model Training\n",
    "    - The logistic regression model is trained using labeled training data. During training, the model learns the optimal coefficients (weights) for each input feature, as well as a bias term. These coefficients are adjusted to minimize a loss function (typically log loss or cross-entropy) that quantifies the difference between predicted probabilities and actual class labels.\n",
    "\n",
    "### Decision Boundary\n",
    "- Logistic Regression generates a decision boundary that separates the two classes in feature space. This boundary is a linear function of input features when using a simple logistic regression model. In more complex scenarios, such as multi-class classification or non-linear relationships, extensions like multinomial logistic regression or polynomial logistic regression are used.\n",
    "\n",
    "### Probability Threshold\n",
    "- Logistic Regression produces probabilities, not discrete class labels. To make predictions, a probability threshold is applied (commonly 0.5). Instances with predicted probabilities greater than the threshold are assigned to the positive class, while those below the threshold are assigned to the negative class.\n",
    "\n",
    "### Evaluation Metrics\n",
    "- To assess the performance of a Logistic Regression model, various evaluation metrics can be used, including accuracy, precision, recall, F1-score, and ROC curves. These metrics provide insights into the model's ability to correctly classify instances and its overall performance.\n",
    "\n",
    "### Regularization\n",
    "- Logistic Regression can be regularized to prevent overfitting. Common regularization techniques include L1 regularization (Lasso) and L2 regularization (Ridge), which add penalty terms to the loss function to constrain the magnitudes of the coefficients.\n",
    "\n",
    "### Use Cases\n",
    "- Logistic Regression finds applications in a wide range of domains, including medical diagnosis, spam detection, credit scoring, and sentiment analysis, to name a few. It is especially useful when the outcome variable is binary, and the goal is to model the probability of an event.\n",
    "\n",
    "Logistic Regression is a powerful and interpretable algorithm that serves as a foundational building block in the world of machine learning. Understanding its principles and applications is essential for anyone working in data science and predictive modeling.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f776fdd4a5f6f10"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logre = LogisticRegression()\n",
    "logre.fit(X_train_scaled,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.771781Z"
    }
   },
   "id": "b2472c654d81fc1c"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "y_pred = logre.predict(X_test_scaled)\n",
    "actual = []\n",
    "prediction = []\n",
    "for i,j in zip(y_test,y_pred):\n",
    "    actual.append(i)\n",
    "    prediction.append(j)\n",
    "dic = {'Actual':actual,\n",
    "       'Prediction':prediction\n",
    "       }\n",
    "result  = pd.DataFrame(dic)\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(0,len(y_test)), y=y_test,\n",
    "                         mode='markers+lines',\n",
    "                         name='Test'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(0,len(y_test)), y=y_pred,\n",
    "                         mode='markers',\n",
    "                         name='Pred'))\n",
    "\n",
    "#fig.write_image('images/logistic_regression.png')\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.773203Z"
    }
   },
   "id": "681e52214dd7a35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualization of Predicted vs. Actual Values\n",
    "\n",
    "In the visualization:\n",
    "\n",
    "- The **red dots** represent the predicted values, which can be either 0 or 1.\n",
    "- The **blue line** and **blue dot** represent the actual values corresponding to specific patients.\n",
    "\n",
    "Where the **red dot** and **blue dot** do not overlap, it indicates incorrect predictions. Conversely, where both the **red dot** and **blue dot** overlap, those are the instances where the model made correct predictions.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "663fab36d1f423f7"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n",
    "classifier_summary_df = add_to_classifier_summary_df(None, logre, \"Logistic Regression\", X_test_scaled, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.774749Z"
    }
   },
   "id": "a80c601e6dad4b0c"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.776017Z"
    }
   },
   "id": "dcae5b69fe19abae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Interpretation of Classification Metrics\n",
    "\n",
    "In the provided classification metrics, we have several key performance indicators for a binary classification model. Let's break down and expand upon each of these metrics:\n",
    "\n",
    "### Precision\n",
    "- **Precision** measures the accuracy of positive predictions made by the model. In this context, it indicates how many of the predicted positive (class 1) instances are actually true positives. A higher precision means that the model is better at avoiding false positives.\n",
    "\n",
    "### Recall\n",
    "- **Recall**, also known as sensitivity or true positive rate, quantifies the model's ability to correctly identify all positive instances in the dataset. It measures the percentage of actual positive (class 1) instances that the model correctly predicted as positive. A higher recall means that the model is better at capturing all actual positives.\n",
    "\n",
    "### F1-Score\n",
    "- The **F1-Score** is the harmonic mean of precision and recall. It provides a balanced measure of a model's accuracy in both precision and recall. A higher F1-Score indicates a good balance between precision and recall.\n",
    "\n",
    "### Support\n",
    "- **Support** refers to the number of samples in each class (0 and 1) in the test dataset. In this case:\n",
    "  - Class 0 has 42 instances.\n",
    "  - Class 1 has 33 instances.\n",
    "\n",
    "### Accuracy\n",
    "- **Accuracy** represents the overall correctness of the model's predictions. It calculates the ratio of correctly predicted instances (both true positives and true negatives) to the total number of instances in the dataset. In this case, the model's accuracy is 0.88, which means it correctly predicts 88% of the test dataset.\n",
    "\n",
    "### Macro Average\n",
    "- The **macro average** is the average of precision, recall, and F1-Score across both classes (0 and 1). It provides an overall summary of the model's performance without considering class imbalances.\n",
    "\n",
    "### Weighted Average\n",
    "- The **weighted average** is similar to the macro average but takes class imbalances into account. It gives more weight to the class with a larger number of samples. In this case, it considers the distribution of class 0 and class 1 instances.\n",
    "\n",
    "In summary, these classification metrics provide a comprehensive evaluation of the model's performance in distinguishing between two classes (0 and 1). They help assess precision, recall, F1-Score, and accuracy, offering insights into the model's strengths and weaknesses in making predictions on the given test dataset.\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "The model has a good overall accuracy of 88%.\n",
    "For class 0 (No Heart Disease), the model achieves high precision (0.87) and recall (0.93), indicating that it is good at correctly identifying both positive and negative examples.\n",
    "For class 1 (Heart Disease), the precision is slightly higher than recall, suggesting that the model is more likely to correctly identify true positives than miss actual cases.\n",
    "The F1-score, which considers both precision and recall, is also high for both classes, indicating that the model is performing well overall.\n",
    "The macro and weighted averages are similar to the overall accuracy, suggesting that the model performs consistently across both classes.\n",
    "\n",
    "Further Analysis:\n",
    "The model shows potential for diagnosing heart disease with good accuracy and performance metrics."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50c029efac8c8a59"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "mpl.rcdefaults()\n",
    "ConfusionMatrixDisplay.from_estimator(logre, X_test_scaled, y_test, display_labels=[\"Does not have HD\", \"Has HD\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.777209Z"
    }
   },
   "id": "15f0c787723bb424"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KNN Classifier\n",
    "\n",
    "The K-Nearest Neighbors (KNN) classifier is a supervised machine learning technique used for classification and regression tasks. Here's a breakdown of how the KNN algorithm works:\n",
    "\n",
    "1. **Initialization of K**: Begin by specifying the number of neighbors (K) that the model should consider when making predictions. K represents the count of nearest data points that will influence the prediction.\n",
    "\n",
    "2. **Prediction Process**: When the KNN model receives an input (real-world data point) as a query, it aims to predict whether the individual has heart disease or not. To do this, it calculates the distance or similarity between the new, unseen data point and all other records in the training dataset.\n",
    "\n",
    "3. **Distance Calculation**: The model computes the distance (e.g., Euclidean distance, Manhattan distance, etc.) between the query data point and each data point in the training set. This distance measurement quantifies how similar or dissimilar the query point is to other data points in the feature space.\n",
    "\n",
    "4. **Sorting by Distance**: After calculating the distances, the model sorts the examples in ascending order, placing the data points with the shortest distances at the beginning of the collection.\n",
    "\n",
    "5. **K-Nearest Data Points**: The KNN algorithm selects the first K entries from the sorted collection. These are the K nearest data points to the query point, based on the calculated distances.\n",
    "\n",
    "6. **Classification or Regression**: The algorithm's behavior depends on the task:\n",
    "   - For regression tasks, KNN returns the \"mean\" of the target values associated with the selected K entries. This average serves as the predicted value for the query point.\n",
    "   - For classification tasks, KNN returns the \"mode\" of the K class labels from the selected entries. In other words, it determines the most frequently occurring class label among the K nearest neighbors. This mode becomes the predicted class for the query point.\n",
    "\n",
    "7. **Model Accuracy Score**: Finally, to assess how well the KNN model performs in real-world scenarios, you can calculate its accuracy score. This metric measures the model's ability to make correct predictions. It is typically done by comparing the predicted results to the actual outcomes in a test dataset.\n",
    "\n",
    "The KNN algorithm is intuitive and relies on the idea that similar data points in the feature space tend to have similar outcomes. By considering the K nearest neighbors, the algorithm makes predictions based on the majority class or average value among those neighbors, making it a versatile and straightforward machine learning approach."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "258f96a84544b34b"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "classifier_summary_df = add_to_classifier_summary_df(classifier_summary_df, knn, \"K Neighbors Classifier - K = 5\", X_test_scaled, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.778757Z"
    }
   },
   "id": "5df47d6a9e3dcd75"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "mpl.rcdefaults()\n",
    "ConfusionMatrixDisplay.from_estimator(knn, X_test_scaled, y_test, display_labels=[\"Does not have HD\", \"Has HD\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.780339Z"
    }
   },
   "id": "9852e64072158482"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "classifier_summary_df = add_to_classifier_summary_df(classifier_summary_df, knn, \"K Neighbors Classifier - K = 7\", X_test_scaled, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.781816Z"
    }
   },
   "id": "c0143fd3216545c0"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "mpl.rcdefaults()\n",
    "ConfusionMatrixDisplay.from_estimator(knn, X_test_scaled, y_test, display_labels=[\"Does not have HD\", \"Has HD\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.783279Z"
    }
   },
   "id": "e75b67df0f5bfa18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "In machine learning, a Decision Tree Classifier is a powerful algorithm used for both classification and regression tasks. It is a tree-like structure where each internal node represents a feature (attribute), each branch represents a decision rule, and each leaf node represents an outcome (class label or predicted value). Decision trees are interpretable models that can be visualized to understand the decision-making process.\n",
    "\n",
    "To create a Decision Tree Classifier in Python, we can use the `DecisionTreeClassifier` class from the scikit-learn library (imported as `sklearn.tree`).\n",
    "\n",
    "Here are the key steps to create a Decision Tree Classifier:\n",
    "\n",
    "1. **Data Preparation:** Ensure that your dataset is properly cleaned, preprocessed, and split into training and testing sets.\n",
    "\n",
    "2. **Import Libraries:** Import the necessary libraries, including `DecisionTreeClassifier` from `sklearn.tree`.\n",
    "\n",
    "3. **Instantiate the Classifier:** Create an instance of the `DecisionTreeClassifier` class, optionally specifying hyperparameters such as the maximum depth of the tree, the minimum number of samples required to split a node, and others.\n",
    "\n",
    "4. **Model Training:** Fit the Decision Tree Classifier to the training data using the `.fit()` method. The classifier learns the decision rules from the training data.\n",
    "\n",
    "5. **Model Evaluation:** After training, you can evaluate the model's performance on a separate testing dataset using various metrics such as accuracy, precision, recall, F1-score, and more.\n",
    "\n",
    "6. **Visualization (Optional):** Decision trees can be visualized using graph visualization tools like Graphviz or by utilizing the built-in plotting capabilities of scikit-learn.\n",
    "\n",
    "7. **Prediction:** Once trained, you can use the Decision Tree Classifier to make predictions on new, unseen data using the `.predict()` method.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9b1654a7df69577"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "# Create Decision Tree classifier object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifier\n",
    "clf = clf.fit(X_train_scaled,y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "classifier_summary_df = add_to_classifier_summary_df(classifier_summary_df, clf, \"Decision Tree Classifier - Gini\", X_test_scaled, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.785009Z"
    }
   },
   "id": "72019d2a9f3071f9"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "feature_cols = ['id', 'age', 'trestbps', 'chol', 'fbs', 'thalch', 'exang', 'oldpeak',\n",
    "                'ca', 'sex_Female', 'sex_Male', 'cp_asymptomatic', 'cp_atypical angina',\n",
    "                'cp_non-anginal', 'cp_typical angina', 'restecg_lv hypertrophy',\n",
    "                'restecg_normal', 'restecg_st-t abnormality', 'slope_downsloping',\n",
    "                'slope_flat', 'slope_upsloping', 'thal_fixed defect', 'thal_normal',\n",
    "                'thal_reversable defect']\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,\n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols  ,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "#graph.write_png('heart-decision-tree.png')\n",
    "Image(graph.create_png())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.786373Z"
    }
   },
   "id": "fa8086e73b698940"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "mpl.rcdefaults()\n",
    "ConfusionMatrixDisplay.from_estimator(clf, X_test_scaled, y_test, display_labels=[\"Does not have HD\", \"Has HD\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.793234Z"
    }
   },
   "id": "120c91515ded9748"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Explanation\n",
    "\n",
    "In the code snippet, we specify two critical hyperparameters when creating the Decision Tree Classifier:\n",
    "\n",
    "#### Criterion (\"entropy\" vs. \"gini\")\n",
    "\n",
    "The `criterion` hyperparameter determines the method used to measure the quality of splits made by the decision tree. In this specific case, we've chosen `\"entropy\"` as the criterion:\n",
    "\n",
    "- **Entropy Criterion**: It quantifies the amount of information disorder or randomness in a dataset. The decision tree aims to minimize entropy by making splits that result in more homogeneous groups (i.e., groups with similar class labels). In simpler terms, the entropy criterion measures the information gain associated with a particular split. It is well-suited for decision trees when you want to maximize the information gained at each split.\n",
    "\n",
    "Another commonly used option is `\"gini\"`:\n",
    "\n",
    "- **Gini Impurity**: Gini impurity measures the probability of incorrectly classifying a randomly chosen element's class label. Similar to entropy, the decision tree seeks to reduce Gini impurity by making splits that result in more homogeneous groups. The \"gini\" criterion is another valid choice for measuring the quality of splits, especially when the goal is classification purity.\n",
    "\n",
    "The choice between \"entropy\" and \"gini\" often depends on the specific problem and the dataset. In practice, both criteria work well, and you may experiment with both to determine which one yields better results for your task.\n",
    "\n",
    "#### Max Depth (Preventing Overfitting)\n",
    "\n",
    "The `max_depth` hyperparameter plays a crucial role in controlling the complexity of the decision tree. It represents the maximum depth or levels the tree can grow to during training. In this code, we set `max_depth=3`, meaning the tree is limited to a depth of 3.\n",
    "\n",
    "- **Preventing Overfitting**: Limiting the tree's depth is a common technique to prevent overfitting. Overfitting occurs when the tree becomes too complex and fits the training data too closely, capturing noise rather than genuine patterns. By restricting the depth, the model becomes less likely to overfit because it is simpler and captures more generalized relationships in the data.\n",
    "\n",
    "The choice of an appropriate `max_depth` value depends on the dataset's complexity and the trade-off between model simplicity and performance. A smaller `max_depth` value results in a simpler model, while a larger value allows the model to capture more intricate patterns in the data. Tuning this hyperparameter is an essential part of optimizing decision tree models.\n",
    "\n",
    "In summary, when configuring a Decision Tree Classifier, the choice of hyperparameters like `criterion` and `max_depth` influences how the tree makes decisions and controls its complexity. Careful consideration of these hyperparameters is vital to achieving the best performance for a given problem.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "956f544a7a4d3165"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "# Create Decision Tree Classifier object\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "# Train Decision Tree Classifier\n",
    "clf = clf.fit(X_train_scaled,y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "classifier_summary_df = add_to_classifier_summary_df(classifier_summary_df, clf, \"Decision Tree Classifier - Entropy\", X_test_scaled, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.793451Z"
    }
   },
   "id": "1ede6c45e1ac7b8a"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,\n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols  ,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "#graph.write_png('heart-decision-tree-2.png')\n",
    "Image(graph.create_png())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.794162Z"
    }
   },
   "id": "9e00ad58fcff237b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb1ae531377808ed"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=100) # , max_depth=5, random_state=1\n",
    "rfc.fit(X_train_scaled, y_train)\n",
    "y_pred = rfc.predict(X_test_scaled)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "classifier_summary_df = add_to_classifier_summary_df(classifier_summary_df, rfc, \"Random Forest Classifier\", X_test_scaled, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.796707Z"
    }
   },
   "id": "3c4bf777700870b2"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.797885Z"
    }
   },
   "id": "4d93e70932697eb6"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "mpl.rcdefaults()\n",
    "ConfusionMatrixDisplay.from_estimator(rfc, X_test_scaled, y_test, display_labels=[\"Does not have HD\", \"Has HD\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.799124Z"
    }
   },
   "id": "f05832c7244de789"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<a id=\"build-svm\"></a>\n",
    "# Building a Preliminary Support Vector Machine (SVM)\n",
    "\n",
    "After correctly formatting the data, we are now ready to build a **Support Vector Machine (SVM)**. Let's proceed with it!\n",
    "\n",
    "In machine learning, a Support Vector Machine (SVM) is a powerful algorithm used for both classification and regression tasks. Building a preliminary SVM involves several essential steps, which we will expand upon:\n",
    "\n",
    "### Instantiating the SVM Classifier\n",
    "Once the data is ready and libraries are imported, you can instantiate the SVM classifier. You may need to specify hyperparameters, such as the type of kernel (e.g., linear, polynomial, radial basis function), regularization parameters (C), and others, depending on your problem and dataset characteristics.\n",
    "\n",
    "### Model Training\n",
    "The next step is to train the SVM classifier using the training dataset. The SVM learns the optimal decision boundary that best separates the different classes in the data. The choice of kernel and hyperparameters plays a critical role in this learning process.\n",
    "\n",
    "### Model Evaluation\n",
    "After training the SVM, it's essential to evaluate its performance. Common evaluation metrics for classification tasks include accuracy, precision, recall, F1-score, and confusion matrix analysis. These metrics help assess how well the SVM can make predictions on unseen data.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "Fine-tuning hyperparameters is often necessary to optimize the SVM model's performance. You can perform hyperparameter tuning using techniques like cross-validation or grid search to find the best combination of hyperparameters that yield the highest predictive accuracy.\n",
    "\n",
    "### Visualization (Optional)\n",
    "Visualizing the SVM's decision boundary can be insightful. For linear SVMs, you can plot the decision boundary in two dimensions, while for non-linear kernels, it may involve more complex visualizations.\n",
    "\n",
    "### Prediction\n",
    "Once your SVM model is trained and validated, you can use it to make predictions on new, unseen data. This step is crucial for deploying the model in real-world applications.\n",
    "\n",
    "Building a preliminary Support Vector Machine is a fundamental process in machine learning that requires careful data preparation, model instantiation, training, evaluation, and optimization. The resulting SVM model can be a valuable tool for solving classification and regression problems across various domains.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5234f6331edca27"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "clf_svm = SVC(random_state=42)\n",
    "clf_svm.fit(X_train_scaled, y_train)\n",
    "classifier_summary_df = add_to_classifier_summary_df(classifier_summary_df, clf_svm, \"Support Vector Machine - Preliminary\", X_test_scaled, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.800710Z"
    }
   },
   "id": "f564dd68a61a8b92"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "mpl.rcdefaults()\n",
    "ConfusionMatrixDisplay.from_estimator(clf_svm, X_test_scaled, y_test, display_labels=[\"Does not have HD\", \"Has HD\"])    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.801580Z"
    }
   },
   "id": "650f8cc1297e61c5"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.803204Z"
    }
   },
   "id": "1a0e9c6ebb54dde0"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "num_features = np.size(X_train_scaled, axis=1)\n",
    "param_grid = [\n",
    "    {'C': [0.1, 0.01, 10, 100, 1000],\n",
    "     'gamma': [1/num_features, 1, 0.1, 0.01, 0.001, 0.0001],\n",
    "     'kernel': ['rbf']},\n",
    "]\n",
    "optimal_params = GridSearchCV(\n",
    "    SVC(),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc', # NOTE: The default value for scoring results in worse performance...\n",
    "    ## For more scoring metrics see: \n",
    "    ## https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    verbose=0 # NOTE: If you want to see what Grid Search is doing, set verbose=2\n",
    ")\n",
    "\n",
    "optimal_params.fit(X_train_scaled, y_train)\n",
    "print(optimal_params.best_params_)\n",
    "c_opt = optimal_params.best_params_['C']\n",
    "gamma_opt = optimal_params.best_params_['gamma']\n",
    "kernel_opt = optimal_params.best_params_['kernel']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.804366Z"
    }
   },
   "id": "ded8ebbd6c155463"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "clf_svm = SVC(random_state=42, C=10, gamma=gamma_opt, kernel=kernel_opt)\n",
    "clf_svm.fit(X_train_scaled, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.805688Z"
    }
   },
   "id": "2b931db23e1b2431"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(clf_svm,\n",
    "                                      X_test_scaled,\n",
    "                                      y_test,\n",
    "                                      display_labels=[\"Does not have HD\", \"Has HD\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.806858Z"
    }
   },
   "id": "b628d6e26f3fbcad"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "classifier_summary_df = add_to_classifier_summary_df(classifier_summary_df, clf_svm, \"Support Vector Machine - Optimal\", X_test_scaled, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.808423Z"
    }
   },
   "id": "45c7c8d8b797f492"
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "pca = PCA() # NOTE: By default, PCA() centers the data, but does not scale it.\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n",
    "labels = [str(x) for x in range(1, len(per_var)+1)]\n",
    "\n",
    "plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.809249Z"
    }
   },
   "id": "c60a7c284b864166"
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "train_pc1_coords = X_train_pca[:, 0]\n",
    "train_pc2_coords = X_train_pca[:, 1]\n",
    "\n",
    "## NOTE:\n",
    "## pc1 contains the x-axis coordinates of the data after PCA\n",
    "## pc2 contains the y-axis coordinates of the data after PCA\n",
    "\n",
    "## Now center and scale the PCs...\n",
    "pca_train_scaled = preprocessing.scale(np.column_stack((train_pc1_coords, train_pc2_coords)))\n",
    "\n",
    "## Now we optimize the SVM fit to the x and y-axis coordinates\n",
    "## of the data after PCA dimension reduction...\n",
    "num_features = np.size(pca_train_scaled, axis=1)\n",
    "param_grid = [\n",
    "    {'C': [100, 1000],\n",
    "     'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "     'kernel': ['rbf']},\n",
    "]\n",
    "\n",
    "optimal_params = GridSearchCV(\n",
    "    SVC(),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc', # NOTE: The default value for scoring results in worse performance...\n",
    "    ## For more scoring metics see: \n",
    "    ## https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    verbose=0 # NOTE: If you want to see what Grid Search is doing, set verbose=2\n",
    ")\n",
    "\n",
    "optimal_params.fit(pca_train_scaled, y_train)\n",
    "print(optimal_params.best_params_)\n",
    "#c_opt = optimal_params.best_params_['C']\n",
    "#gamma_opt = optimal_params.best_params_['gamma']\n",
    "#kernel_opt = optimal_params.best_params_['kernel']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.810554Z"
    }
   },
   "id": "26580953fcbaf1b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decision Surface Plot with SVM\n",
    "\n",
    "The code snippet presented below generates a decision surface plot using a Support Vector Machine (SVM) classifier. The process includes several key steps:\n",
    "\n",
    "1. **Classifier Training**: We first train the SVM classifier, configuring its hyperparameters and fitting it to the training data.\n",
    "\n",
    "2. **Test Data Transformation**: The test dataset is transformed using the same Principal Component Analysis (PCA) transformation applied to the training data. This ensures consistency in feature scaling and dimensionality reduction.\n",
    "\n",
    "3. **Mesh Grid Creation**: We create a mesh grid to visualize the decision regions. The grid covers the range of possible feature values, and each point in the grid represents a potential data point.\n",
    "\n",
    "4. **Point Classification**: The SVM classifier is used to classify each point in the mesh grid. This step involves predicting the class label (0 or 1) for every grid point based on the learned decision boundaries.\n",
    "\n",
    "5. **Visualization**: The results are plotted, with filled contour regions representing the decision regions determined by the SVM. Additionally, the actual data points from the test dataset are displayed on the plot, colored according to their known classifications.\n",
    "\n",
    "This decision surface plot serves as a visual aid to understand how the SVM model separates the two classes within the feature space, providing insights into the model's performance and the boundaries it establishes.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb88513a34622a17"
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "# Creating Decision Surface Plot with SVM\n",
    "\n",
    "# Create an SVM classifier with specified hyperparameters\n",
    "clf_svm = SVC(random_state=42, C=c_opt, gamma=gamma_opt, kernel=kernel_opt)\n",
    "\n",
    "# Train the SVM classifier on the scaled and PCA-transformed training data\n",
    "clf_svm.fit(pca_train_scaled, y_train)\n",
    "\n",
    "# Transform the test dataset using the same PCA transformation\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "test_pc1_coords = X_test_pca[:, 0]\n",
    "test_pc2_coords = X_test_pca[:, 1]\n",
    "\n",
    "# Define the range for the decision surface matrix\n",
    "x_min = test_pc1_coords.min() - 1\n",
    "x_max = test_pc1_coords.max() + 1\n",
    "y_min = test_pc2_coords.min() - 1\n",
    "y_max = test_pc2_coords.max() + 1\n",
    "\n",
    "# Create a mesh grid for the decision regions\n",
    "xx, yy = np.meshgrid(np.arange(start=x_min, stop=x_max, step=0.1),\n",
    "                     np.arange(start=y_min, stop=y_max, step=0.1))\n",
    "\n",
    "# Classify each point in the mesh grid using the SVM\n",
    "Z = clf_svm.predict(np.column_stack((xx.ravel(), yy.ravel())))\n",
    "\n",
    "# Reshape the classification results to match the mesh grid dimensions\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Create a figure and axis for the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot filled contour regions based on the SVM classifications\n",
    "ax.contourf(xx, yy, Z, alpha=0.1)\n",
    "\n",
    "# Define custom colors for the actual data points\n",
    "cmap = colors.ListedColormap(['#e41a1c', '#4daf4a'])\n",
    "\n",
    "# Plot the actual data points with their known classifications\n",
    "scatter = ax.scatter(test_pc1_coords, test_pc2_coords, c=y_test,\n",
    "                     cmap=cmap,\n",
    "                     s=100,\n",
    "                     edgecolors='k',  # 'k' represents black\n",
    "                     alpha=0.7)\n",
    "\n",
    "# Create a legend for the data points\n",
    "legend = ax.legend(scatter.legend_elements()[0],\n",
    "                   scatter.legend_elements()[1],\n",
    "                   loc=\"upper right\")\n",
    "legend.get_texts()[0].set_text(\"Does not have HD\")\n",
    "legend.get_texts()[1].set_text(\"Have HD\")\n",
    "\n",
    "# Add axis labels and a title to the plot\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_title('Decision Surface using PCA-transformed Features')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.811729Z"
    }
   },
   "id": "ca1a6d751c6bb15"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(16, 6))\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"Macro F1\"]\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axs[i].bar(classifier_summary_df[\"Classifier Name\"], classifier_summary_df[metric])\n",
    "    axs[i].set_title(metric)\n",
    "    axs[i].set_xlabel(\"Model\")\n",
    "    axs[i].set_ylabel(\"Score\")\n",
    "    axs[i].set_ylim(0.7, 1.0)\n",
    "    axs[i].tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.813342Z"
    }
   },
   "id": "e59dc2a3910b0271"
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "classifier_summary_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.817507Z"
    }
   },
   "id": "1aff1019ca94055a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Evaluation and Conclusion\n",
    "\n",
    "In this section, we present the detailed meaning and interpretation of the evaluation results for various machine learning models. The results include accuracy, precision, recall, and F1-Score metrics for each model. Let's delve into the analysis of these results:\n",
    "\n",
    "1. **Logistic Regression**:\n",
    "- **Accuracy**: 0.893333\n",
    "- Meaning: The Logistic Regression model achieved an accuracy of approximately 89.33%, indicating that it correctly predicted the class labels for nearly 89.33% of the test data instances.\n",
    "- **Precision**: 0.894795\n",
    "- Meaning: The precision of around 89.48% suggests that when the model predicted the positive class, it was correct approximately 89.48% of the time.\n",
    "- **Recall**: 0.888528\n",
    "- Meaning: The recall of roughly 88.85% indicates that the model effectively captured nearly 88.85% of the actual positive instances.\n",
    "- **F1-Score**: 0.890988\n",
    "- Meaning: The F1-Score, which combines precision and recall, is approximately 89.10%. This balanced metric reflects the overall performance of the model.\n",
    "\n",
    "2. **K Neighbors Classifier - K = 5**:\n",
    "- This model outperformed Logistic Regression in all metrics, demonstrating higher accuracy, precision, recall, and F1-Score. With an accuracy of around 90.67%, it correctly predicted class labels for the majority of test instances.\n",
    "\n",
    "3. **K Neighbors Classifier - K = 7**:\n",
    "- This model further improved on K Neighbors Classifier with K = 5, achieving even higher accuracy (approximately 93.33%), precision, recall, and F1-Score.\n",
    "\n",
    "4. **Decision Tree Classifier**:\n",
    "- This model showed lower performance compared to the K Neighbors models and Logistic Regression, with an accuracy, precision, recall, and F1-Score of approximately 78.67%.\n",
    "\n",
    "5. **Decision Tree Classifier - Gini**:\n",
    "- This model performed slightly worse than the regular Decision Tree Classifier, with an accuracy, precision, recall, and F1-Score of around 76.00%.\n",
    "\n",
    "6. **Random Forest Classifier - Entropy**:\n",
    "- The Random Forest model with entropy achieved an accuracy of approximately 88.00%, similar to Logistic Regression, and demonstrated a balanced performance in terms of precision, recall, and F1-Score.\n",
    "\n",
    "7. **Support Vector Machine - Preliminary**:\n",
    "- This SVM model showed similar performance to K Neighbors Classifier with K = 5, with an accuracy, precision, recall, and F1-Score of around 90.67%.\n",
    "\n",
    "8. **Support Vector Machine - Optimal**:\n",
    "- The optimal SVM model achieved an accuracy of approximately 88.00%, similar to Random Forest, with competitive precision, recall, and F1-Score.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Based on the evaluation results, we can draw the following conclusions:\n",
    "\n",
    "- **K Neighbors Classifier with K = 7** stands out as the best-performing model, achieving the highest accuracy (approximately 93.33%) and balanced precision, recall, and F1-Score metrics. This model is recommended for this specific classification task.\n",
    "\n",
    "- **K Neighbors Classifier with K = 5** and **Support Vector Machine - Preliminary** also demonstrated strong performance, with accuracy around 90.67%.\n",
    "\n",
    "- **Logistic Regression**, **Random Forest Classifier - Entropy**, and **Support Vector Machine - Optimal** performed reasonably well with accuracies around 88.00%.\n",
    "\n",
    "- The decision tree-based models, including **Decision Tree Classifier** and **Decision Tree Classifier - Gini**, showed lower accuracy and overall performance compared to other models.\n",
    "\n",
    "In summary, the choice of the best model depends on specific project requirements and the importance of different metrics. K Neighbors Classifier with K = 7 offers the highest accuracy and well-balanced precision and recall, making it a strong candidate for deployment in this classification task.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43ec3c6f3b603029"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T23:32:44.816150Z"
    }
   },
   "id": "c6358b9ef5dc04b7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
